{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file, we are running MPII testing dataset consisting of 24984 images and SURREAL video dataset using MPII model and COCO testing data using COCO model, measuring the time it takes as an analytical parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# if not os.path.exists(output_dir_images):\n",
    "#     os.makedirs(output_dir_images)\n",
    "    \n",
    "# if not os.path.exists(output_dir_videos):\n",
    "#     os.makedirs(output_dir_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Specify the model to be used:\n",
    "## Let's go with MPII first \n",
    "COCO and MPII are body pose estimation model. COCO has 18 points and MPI has 15 points as output.\n",
    "### Load the network and specify the image dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mpii_model():\n",
    "    protoFile = \"pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt\"\n",
    "    weightsFile = \"pose/mpi/pose_iter_160000.caffemodel\"\n",
    "    nPoints = 15\n",
    "    POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]\n",
    "    colors = [ [255,255,0],[0,100,255], [0,255,255], [0,100,255],[0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [0,255,0]]\n",
    "    \n",
    "    net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "    inWidth = 368\n",
    "    inHeight = 368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the keypoints for an image with only single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_mpii_model(data_dir_images, output_dir_images, file):\n",
    "\n",
    "    \n",
    "    \n",
    "    for img_file in os.listdir(data_dir_images):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(data_dir_images, img_file)\n",
    "            frame = cv2.imread(img_path)\n",
    "            frameCopy = np.copy(frame)\n",
    "            frameWidth = frame.shape[1]\n",
    "            frameHeight = frame.shape[0]\n",
    "            threshold = 0.1\n",
    "\n",
    "            inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                              (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "            net.setInput(inpBlob)\n",
    "\n",
    "            output = net.forward()\n",
    "            H = output.shape[2]\n",
    "            W = output.shape[3]\n",
    "\n",
    "            # Empty list to store the detected keypoints\n",
    "            points = []\n",
    "\n",
    "\n",
    "            for i in range(nPoints):\n",
    "                # confidence map of corresponding body's part.\n",
    "                probMap = output[0, i, :, :]\n",
    "\n",
    "                # Find global maxima of the probMap.\n",
    "                minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "                # Scale the point to fit on the original image\n",
    "                x = (frameWidth * point[0]) / W\n",
    "                y = (frameHeight * point[1]) / H\n",
    "\n",
    "                if prob > threshold : \n",
    "                    cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "                    cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "                    cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "                    # Add the point to the list if the probability is greater than the threshold\n",
    "                    points.append((int(x), int(y)))\n",
    "                else :\n",
    "                    points.append(None)\n",
    "\n",
    "            # Draw Skeleton\n",
    "            for pair in POSE_PAIRS:\n",
    "                partA = pair[0]\n",
    "                partB = pair[1]\n",
    "\n",
    "                if points[partA] and points[partB]:\n",
    "                    #cv2.line(frame, points[partA], points[partB], (0,255,255), 3)\n",
    "                    cv2.line(frame, points[partA], points[partB], colors[POSE_PAIRS.index(pair)], 3)\n",
    "                    cv2.circle(frame, points[partA], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "                    cv2.circle(frame, points[partB], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "\n",
    "            # Save the processed image to the output directory\n",
    "            file_name = os.path.basename(img_path)\n",
    "            output_path = os.path.join(output_dir_images, file_name)\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            with open(file, 'a') as f:\n",
    "                f.write(f'{img_file}, {points}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the keypoints for a video with only single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_mpii_model(data_dir_videos, output_dir_videos, file):\n",
    "\n",
    "    for vid_file in os.listdir(data_dir_videos):\n",
    "        if vid_file.endswith(\".mp4\"):\n",
    "            vid_path = os.path.join(data_dir_videos, vid_file)\n",
    "            cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "            file_name = os.path.basename(vid_path)\n",
    "            output_path = os.path.join(output_dir_videos, file_name)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)),int(cap.get(4))))\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frameCopy = np.copy(frame)\n",
    "                frameWidth = frame.shape[1]\n",
    "                frameHeight = frame.shape[0]\n",
    "                threshold = 0.1\n",
    "\n",
    "                inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                                          (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "                net.setInput(inpBlob)\n",
    "\n",
    "                output = net.forward()\n",
    "                H = output.shape[2]\n",
    "                W = output.shape[3]\n",
    "\n",
    "                # Empty list to store the detected keypoints\n",
    "                points = []\n",
    "\n",
    "                for i in range(nPoints):\n",
    "                    # confidence map of corresponding body's part.\n",
    "                    probMap = output[0, i, :, :]\n",
    "\n",
    "                    # Find global maxima of the probMap.\n",
    "                    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "                    # Scale the point to fit on the original image\n",
    "                    x = (frameWidth * point[0]) / W\n",
    "                    y = (frameHeight * point[1]) / H\n",
    "\n",
    "                    if prob > threshold : \n",
    "                        cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "                        cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "                        cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "                        # Add the point to the list if the probability is greater than the threshold\n",
    "                        points.append((int(x), int(y)))\n",
    "                    else :\n",
    "                        points.append(None)\n",
    "\n",
    "                # Draw Skeleton\n",
    "                for pair in POSE_PAIRS:\n",
    "                    partA = pair[0]\n",
    "                    partB = pair[1]\n",
    "\n",
    "                    if points[partA] and points[partB]:\n",
    "                        cv2.line(frame, points[partA], points[partB], colors[POSE_PAIRS.index(pair)], 3)\n",
    "                        cv2.circle(frame, points[partA], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "                        cv2.circle(frame, points[partB], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "\n",
    "                #Save the processed image to the output directory\n",
    "\n",
    "                #cv2.imwrite(output_path, frame)\n",
    "                out.write(cv2.resize(frame, (int(cap.get(3)),int(cap.get(4)))))\n",
    "                # with open(file, 'a') as f:\n",
    "                #     f.write(f'{img_file}, {points}\\n')\n",
    "\n",
    "    # Release the VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on COCO images\n",
    "### Load the network and specify the image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_model():\n",
    "    protoFile = \"pose/coco/pose_deploy_linevec.prototxt\"\n",
    "    weightsFile = \"pose/coco/pose_iter_440000.caffemodel\"\n",
    "    nPoints = 18\n",
    "    POSE_PAIRS = [ [1,0],[1,2],[1,5],[2,3],[3,4],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13],[0,14],[0,15],[14,16],[15,17]]\n",
    "    colors = [ [0,0,255],[0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [255,0,0], [255,0,0],[200,200,0],[0,0,0]]\n",
    "\n",
    "    net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "    inWidth = 368\n",
    "    inHeight = 368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the keypoints for an image with only single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def image_coco_model(data_dir_images, output_dir_images, file):\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for img_file in os.listdir(data_dir_images):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(data_dir_images, img_file)\n",
    "            frame = cv2.imread(img_path)\n",
    "            frameCopy = np.copy(frame)\n",
    "\n",
    "            frameWidth = frame.shape[1]\n",
    "            frameHeight = frame.shape[0]\n",
    "            threshold = 0.1\n",
    "\n",
    "            inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                                      (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "            net.setInput(inpBlob)\n",
    "\n",
    "            output = net.forward()\n",
    "            H = output.shape[2]\n",
    "            W = output.shape[3]\n",
    "\n",
    "            # Empty list to store the detected keypoints\n",
    "            points = []\n",
    "\n",
    "\n",
    "            for i in range(nPoints):\n",
    "                # confidence map of corresponding body's part.\n",
    "                probMap = output[0, i, :, :]\n",
    "\n",
    "                # Find global maxima of the probMap.\n",
    "                minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "                # Scale the point to fit on the original image\n",
    "                x = (frameWidth * point[0]) / W\n",
    "                y = (frameHeight * point[1]) / H\n",
    "\n",
    "                if prob > threshold : \n",
    "                    cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "                    cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "                    cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "                    # Add the point to the list if the probability is greater than the threshold\n",
    "                    points.append((int(x), int(y)))\n",
    "                else :\n",
    "                    points.append(None)\n",
    "\n",
    "            # Draw Skeleton\n",
    "            for pair in POSE_PAIRS:\n",
    "                partA = pair[0]\n",
    "                partB = pair[1]\n",
    "\n",
    "                if points[partA] and points[partB]:\n",
    "                    #cv2.line(frame, points[partA], points[partB], (0,255,255), 3)\n",
    "                    cv2.line(frame, points[partA], points[partB], colors[POSE_PAIRS.index(pair)], 3)\n",
    "                    cv2.circle(frame, points[partA], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "                    cv2.circle(frame, points[partB], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "\n",
    "            # Save the processed image to the output directory\n",
    "            file_name = os.path.basename(img_path)\n",
    "            output_path = os.path.join(output_dir_images, file_name)\n",
    "            cv2.imwrite(output_path, frame)\n",
    "            with open(file, 'a') as f:\n",
    "                f.write(f'{img_file}, {points}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the keypoints for a video with only single person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_coco_model(data_dir_videos, output_dir_videos, file):\n",
    "\n",
    "    protoFile = \"pose/coco/pose_deploy_linevec.prototxt\"\n",
    "    weightsFile = \"pose/coco/pose_iter_440000.caffemodel\"\n",
    "    nPoints = 18\n",
    "    POSE_PAIRS = [ [1,0],[1,2],[1,5],[2,3],[3,4],[5,6],[6,7],[1,8],[8,9],[9,10],[1,11],[11,12],[12,13],[0,14],[0,15],[14,16],[15,17]]\n",
    "    colors = [ [0,0,255],[0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\n",
    "         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\n",
    "         [255,0,0], [255,0,0],[200,200,0],[0,0,0]]\n",
    "\n",
    "    net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\n",
    "\n",
    "    inWidth = 368\n",
    "    inHeight = 368\n",
    "    \n",
    "    for vid_file in os.listdir(data_dir_videos):\n",
    "        if vid_file.endswith(\".mp4\"):\n",
    "            vid_path = os.path.join(data_dir_videos, vid_file)\n",
    "            cap = cv2.VideoCapture(vid_path)\n",
    "\n",
    "            file_name = os.path.basename(vid_path)\n",
    "            output_path = os.path.join(output_dir_videos, file_name)\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)),int(cap.get(4))))\n",
    "\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                frameCopy = np.copy(frame)\n",
    "                frameWidth = frame.shape[1]\n",
    "                frameHeight = frame.shape[0]\n",
    "                threshold = 0.1\n",
    "\n",
    "                inpBlob = cv2.dnn.blobFromImage(frame, 1.0 / 255, (inWidth, inHeight),\n",
    "                                          (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "                net.setInput(inpBlob)\n",
    "\n",
    "                output = net.forward()\n",
    "                H = output.shape[2]\n",
    "                W = output.shape[3]\n",
    "\n",
    "                # Empty list to store the detected keypoints\n",
    "                points = []\n",
    "\n",
    "                for i in range(nPoints):\n",
    "                    # confidence map of corresponding body's part.\n",
    "                    probMap = output[0, i, :, :]\n",
    "\n",
    "                    # Find global maxima of the probMap.\n",
    "                    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "                    # Scale the point to fit on the original image\n",
    "                    x = (frameWidth * point[0]) / W\n",
    "                    y = (frameHeight * point[1]) / H\n",
    "\n",
    "                    if prob > threshold : \n",
    "                        cv2.circle(frameCopy, (int(x), int(y)), 8, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "                        cv2.putText(frameCopy, \"{}\".format(i), (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, lineType=cv2.LINE_AA)\n",
    "                        cv2.circle(frame, (int(x), int(y)), 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "                        # Add the point to the list if the probability is greater than the threshold\n",
    "                        points.append((int(x), int(y)))\n",
    "                    else :\n",
    "                        points.append(None)\n",
    "\n",
    "                # Draw Skeleton\n",
    "                for pair in POSE_PAIRS:\n",
    "                    partA = pair[0]\n",
    "                    partB = pair[1]\n",
    "\n",
    "                    if points[partA] and points[partB]:\n",
    "                        cv2.line(frame, points[partA], points[partB], colors[POSE_PAIRS.index(pair)], 3)\n",
    "                        cv2.circle(frame, points[partA], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "                        cv2.circle(frame, points[partB], 8, colors[POSE_PAIRS.index(pair)], thickness=-1)\n",
    "\n",
    "                #Save the processed image to the output directory\n",
    "\n",
    "                #cv2.imwrite(output_path, frame)\n",
    "                out.write(cv2.resize(frame, (int(cap.get(3)),int(cap.get(4)))))\n",
    "\n",
    "    # Release the VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we test the models\n",
    "### We run the MPII model on the MPII image dataset and the COCO image dataset and the SURREAL video dataset\n",
    "\n",
    "### First we run the MPII model on MPII dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_images = \"dataset/single_person/mpii_dataset\"\n",
    "output_dir_images = \"results/single_person/mpii_model/mpii_dataset\"\n",
    "file = \"predicted_keypoints/mpii_model_mpii_dataset.txt\"\n",
    "load_mpii_model()\n",
    "t = time.time()\n",
    "image_mpii_model(data_dir_images, output_dir_images, file)\n",
    "print(\"Time Taken for MPII model running MPII dataset = {}\".format(time.time() - t))\n",
    "\n",
    "# Time Taken = 11689.12948679924"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running MPII model on COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_images = \"dataset/single_person/coco_dataset\"\n",
    "output_dir_images = \"results/single_person/mpii_model/coco_dataset\"\n",
    "file = \"mpii_model_coco_dataset.txt\"\n",
    "#load_mpii_model()\n",
    "t = time.time()\n",
    "image_mpii_model(data_dir_images, output_dir_images, file)\n",
    "print(\"Time Taken for MPII model running COCO dataset = {}\".format(time.time() - t))\n",
    "\n",
    "# Time taken = 19398.382776021957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running MPII model on SURREAL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX PATHS\n",
    "# data_dir_videos = \"dataset/single_person/surreal_dataset\"\n",
    "# output_dir_videos = \"results/single_person/mpii_model/coco_dataset\"\n",
    "# file = \"mpii_model_coco_dataset.txt\"\n",
    "# t = time.time()\n",
    "# video_mpii_model(data_dir_videos, output_dir_videos, file):\n",
    "# print(\"Time Taken for MPII model running COCO dataset = {}\".format(time.time() - t))\n",
    "\n",
    "# 9:22 to 1:32, 362 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running COCO model on MPII dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_images = \"dataset/single_person/mpii_dataset\"\n",
    "output_dir_images = \"results/single_person/coco_model/mpii_dataset\"\n",
    "file = \"predicted_keypoints/coco_model_mpii_dataset.txt\"\n",
    "t = time.time()\n",
    "image_coco_model(data_dir_images, output_dir_images, file)\n",
    "print(\"Time Taken for COCO model running MPII dataset = {}\".format(time.time() - t))\n",
    "\n",
    "# Time Taken = 16239.520328998566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running COCO model on COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_images = \"dataset/single_person/coco_dataset\"\n",
    "output_dir_images = \"results/single_person/coco_model/coco_dataset\"\n",
    "file = \"predicted_keypoints/coco_model_coco_dataset.txt\"\n",
    "t = time.time()\n",
    "image_coco_model(data_dir_images, output_dir_images, file)\n",
    "print(\"Time Taken for COCO model running COCO dataset = {}\".format(time.time() - t))\n",
    "\n",
    "# Time Taken = 36547.80410313606\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running COCO model on SURREAL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpii_model_coco_dataset.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mvideo_coco_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir_videos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir_videos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Taken for coco model running surreal dataset = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t))\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mvideo_coco_model\u001b[0;34m(data_dir_videos, output_dir_videos, file)\u001b[0m\n\u001b[1;32m     36\u001b[0m inpBlob \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mblobFromImage(frame, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m, (inWidth, inHeight),\n\u001b[1;32m     37\u001b[0m                           (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), swapRB\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m net\u001b[38;5;241m.\u001b[39msetInput(inpBlob)\n\u001b[0;32m---> 41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m H \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     43\u001b[0m W \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FIX PATHS\n",
    "data_dir_videos = \"dataset/single_person/surreal_dataset\"\n",
    "output_dir_videos = \"results/single_person/coco_model/surreal_dataset\"\n",
    "file = \"mpii_model_coco_dataset.txt\"\n",
    "t = time.time()\n",
    "video_coco_model(data_dir_videos, output_dir_videos, file)\n",
    "print(\"Time Taken for coco model running surreal dataset = {}\".format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
